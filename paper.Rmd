---
title: "The patient-as-fixed-effect fallacy: Consequences for power and Type I errors"
subtitle: "Examination for the course: 'Open science and reproducible research'"
author: "Kristoffer Magnusson"
date: "March 13, 2018"
output: 
  bookdown::pdf_document2:
    includes:  
      in_header: latex-figure-floats.tex
bibliography: references.bib

---




```{r setup, include=FALSE}
# sim objects
load("data/sim.Rdata")

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      out.width = "0.7\\textwidth",
                      fig.align = "center")

library(powerlmm)
library(ggplot2)
library(tidyr)
library(dplyr)
```

# Introduction
In clinical psychology, and in many other fields, linear mixed-effects models (LMMs) have quickly risen in popularity during the 21st century [@gueorguieva_move_2004]. Their usage in clincal psychology is often motivated by LMMs ability to include participants with missing data [e.g., @kahn_multilevel_2011]. However, LMMs are highly sensitive to some types of model misspecification [@kwok_impact_2007], and investigators are faced with many modelling choices, or researchers degrees of freedom [@wicherts_degrees_2016]. When analyzing patients that have been repeatedy measured during a treatment, investigators must decide wheter to model subjects' change over time as fixed or varying (random). The issues of treating and effect as constant for all individuals (fixed), or as varying between individuals (random), goes back a long time. For instance, in linguistics @clark_language-as-fixed-effect_1973 coined the term "language-as-fixe-effect fallcy", and @martindale_therapist-as-fixed-effect_1978 soon followed by pointing out the unreanable assumption that therapists have excactly the same sucess with their patients ("therapist-as-a-fixed-effect fallacy"). In an influentian simulation study @barr_random_2013 recommended to "keep it maximal", i.e. include as many random effects as possible. Others [e.g., @matuschek_balancing_2017] have noted that keeping it maximal might be too conservative, and that investigators need to balance the risk of Type I or II errors.  

In this paper we will focus on one of the most basic decisions an analyst must make, when analyzing longitudinal treatment data---whether subjects' trajectories over time should be seen as an fixed or random effect. Specifically, this paper fouces on the consequence of ignoring subject-spefic varying slopes, on both Type I and II errors.

# Methods

In typical multilevel notation, the simplest case of the two-level model is,

\begin{align}
\text{Level 1}& \notag\\
Y_{ij} &= \beta_{0j} + \beta_{1j}t_{ij} + R_{ij}\\
\text{Level 2}& \notag\\\
\beta_{0j} &= \gamma_{00} + \gamma_{01} TX_j + U_{0j} \\
\beta_{1j} &= \gamma_{10} + \gamma_{11} TX_j + U_{1j} \\
\end{align}
\begin{equation}
\text{with }
\begin{pmatrix}
U_{0j} \\
 U_{1j}
\end{pmatrix}
\sim\mathcal{N}
\left(
\begin{matrix}
0 &\\\
0
\end{matrix}
,
\begin{matrix}
 \sigma_{u_0}^2 & \sigma_{u_{01}}\\
 \sigma_{u_{01}} & \sigma_{u_1}^2
\end{matrix}
\right)
, \text{ and } R_{ij} \sim\mathcal{N}(0, ~\sigma^2)
\end{equation}


The parameter if interest is $\gamma_{11}$, which is the mean difference in change between the two groups. The aim of this paper is to investigate if accounting for subject-specific slopes ($\sigma_{u_1}^2 > 0$) is important, both when planning the study, or when analyzing the outcome. 


## Simulation
  To investigated the impact of wrongly omitting a random slope on the risk of comitting a Type I error, a Monte Carlo simulation was performed. A parallel group RCT with 11 weekly time points was assumed, with 50 participants in each treatment group. We also assumed that at baseline there was an equal amount of variance between and within subjects, which would translate to an intraclass correlation of 0.5, if there was no variation between subjets in change over time,  $\sigma_{u_1}^2 = 0$. The simulation consistent of comparing 5 different amounts of random slope variance, $\sigma_{u_1}^2/\sigma^2 = \{0, 0.01, 0.02, 0.03, 0.04\}$. We write the sloep variance as a fraction of the error variance, since its the ratio that matters, not the absolute value of, $\sigma_{u_1}^2$.\footnote{Here I use the famous proof: "proof is left as an excercise to the reader", or "It can easily be shown that..." ;)} 

Power was calculated for the same model, assuming a Cohen's *d* of 0.5 (standardized using the pretest standard deviation). Simulations and power calculations were done in R [version 3.4.3; @r_core_team_r:_2017], using *powerlmm* [version 0.2.0; @magnusson_powerlmm:_2018]. LMMs were fit with *lme4* [version 1.1-15; @bates_fitting_2015], using restricted maximum likelihood estimation. For each model 5000 data sets were generated, resulting in a 95 % Monte Carlo CI of 0.044--0.056, for a nominal $\alpha$ of 0.05, and 

# Results

Figure \@ref(fig:type1-error) shows that.
```{r, cache = TRUE}
p <- study_parameters(n1 = 11, 
                      n2 = 50,
                      icc_pre_subject = 0.5,
                      cor_subject = -0.4,
                      var_ratio = c(0, 0.01, 0.02, 0.03, 0.04),
                      cohend = 0.5
                      )

type1 <- summary(res_H0, model = "wrong")
type1$var_ratio <- get_var_ratio(p)
type1$type1_correct <- summary(res_H0, model = "correct")$Power

d <- type1 %>% 
  select(var_ratio, type1_wrong = Power, type1_correct) %>% 
  gather(model, type1, -var_ratio) %>% 
  mutate(model = factor(model, labels = c("Random intercept & slope", "Random intercept-only")))
```

```{r type1-error, fig.cap = "Type I errors for models with different amounts of true slope variance. Nominal levels is 0.05."}
ggplot(d, aes(var_ratio, type1, color = model)) + 
  geom_point() + 
  geom_line() +
  labs(x = "Variance ratio \n(random_slope^2/error^2)",
       y = "Type I Error") +
  geom_hline(yintercept = 0.05, linetype = "dotted") +
  geom_text(aes(label = round(type1, 3)), hjust = 0.5, vjust = -1) + 
  coord_cartesian(ylim = c(0, 0.32)) +
  theme_minimal() +
  theme(legend.position = c(0.85, 0.4))
```



## Power curves
```{r, cache = TRUE}
p <- study_parameters(n1 = 11, 
                      n2 = 50,
                      icc_pre_subject = 0.5,
                      cor_subject = -0.4,
                      var_ratio = c(0, 0.01, 0.02, 0.03, 0.04),
                      cohend = 0.5
                      )

# Power curve
pcurve <- get_power_table(p, n2 = 20:100, var_ratio = c(0, 0.01, 0.02, 0.03, 0.04))
```

```{r power-curve, fig.cap = "Power curves. Cohen's d = 0.5, 11 time points, and ICC at pretest = 0.5."}
plot(pcurve) + 
  geom_vline(xintercept = 100, linetype = "dotted") +
  guides(linetype = "none") +
  labs(color = "Variance ratio \n(random_slope^2/error^2)") +
  theme(legend.position = "bottom") +
  scale_color_viridis_d()
```



# Discussion
Even if som model selection could be used, to select the most parsimonus model, investigators must still decide wheter the test should be sensitive to detect relevant diferences even if random slopes are included. Our recommondetation is that researchers assume participants will change differently during a treatment trial, and include enough participants so that the statistical tests will not miss clinicalyl relevant effects even under moderate to large amounts of heterogeinty in change. To facilitite this we encourage authors to actually report these variance components, so that some kind of reference point is avaiable.

Lastly, our simulation showed that ignoring even small amounts of slopes variance can substantially inflate the Type I errors. Hence, we think it is reasnoable that investigators who concider a random intrecept-only model a reasonable choice, provide sufficient evidence for this asummption.

# References