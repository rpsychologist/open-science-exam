
@misc{magnusson_powerlmm:_2018,
	title = {powerlmm: {Power} analysis for longitudinal multilevel models},
	url = {https://CRAN.R-project.org/package=powerlmm},
	author = {Magnusson, Kristoffer},
	year = {2018}
}

@article{kahn_multilevel_2011,
	title = {Multilevel modeling: overview and applications to research in counseling psychology},
	volume = {58},
	issn = {0022-0167},
	shorttitle = {Multilevel modeling},
	doi = {10.1037/a0022680},
	abstract = {Multilevel modeling (MLM) is rapidly becoming the standard method of analyzing nested data, for example, data from students within multiple schools, data on multiple clients seen by a smaller number of therapists, and even longitudinal data. Although MLM analyses are likely to increase in frequency in counseling psychology research, many readers of counseling psychology journals have had only limited exposure to MLM concepts. This paper provides an overview of MLM that blends mathematical concepts with examples drawn from counseling psychology. This tutorial is intended to be a first step in learning about MLM; readers are referred to other sources for more advanced explorations of MLM. In addition to being a tutorial for understanding and perhaps even conducting MLM analyses, this paper reviews recent research in counseling psychology that has adopted a multilevel framework, and it provides ideas for MLM approaches to future research in counseling psychology.},
	language = {eng},
	number = {2},
	journal = {Journal of Counseling Psychology},
	author = {Kahn, Jeffrey H.},
	month = apr,
	year = {2011},
	pmid = {21463032},
	keywords = {Analysis of Variance, Behavioral Research, Counseling, Humans, Longitudinal Studies, Models, Statistical, Multilevel Analysis, Students},
	pages = {257--271}
}

@misc{r_core_team_r:_2017,
	address = {Vienna, Austria: R Foundation for statistical computing},
	title = {R: {A} language and environment for statistical computing},
	author = {{R Core Team,}},
	year = {2017}
}

@article{bates_fitting_2015,
	title = {Fitting linear mixed-effects models using lme4},
	volume = {67},
	doi = {10.18637/jss.v067.i01},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	year = {2015},
	pages = {1--48}
}

@article{martindale_therapist-as-fixed-effect_1978,
	title = {The therapist-as-fixed-effect fallacy in psychotherapy research},
	volume = {46},
	issn = {0022-006X},
	doi = {10.1037/0022-006X.46.6.1526},
	language = {eng},
	number = {6},
	journal = {Journal of Consulting and Clinical Psychology},
	author = {Martindale, C.},
	year = {1978},
	pmid = {730913},
	keywords = {Factor Analysis, Statistical, Humans, Psychotherapy, Research},
	pages = {1526--1530}
}

@article{matuschek_balancing_2017,
	title = {Balancing {Type} {I} error and power in linear mixed models},
	volume = {94},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X17300013},
	doi = {10.1016/j.jml.2017.01.001},
	abstract = {Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments. Although LMMs have many advantages over ANOVA, like ANOVAs, setting them up for data analysis also requires some care. One simple option, when numerically possible, is to fit the full variance-covariance structure of random effects (the maximal model; Barr, Levy, Scheepers \& Tily, 2013), presumably to keep Type I error down to the nominal α in the presence of random effects. Although it is true that fitting a model with only random intercepts may lead to higher Type I error, fitting a maximal model also has a cost: it can lead to a significant loss of power. We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data, higher power is achieved without inflating Type I error rate if a model selection criterion is used to select a random effect structure that is supported by the data.},
	urldate = {2018-03-13},
	journal = {Journal of Memory and Language},
	author = {Matuschek, Hannes and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald and Bates, Douglas},
	month = jun,
	year = {2017},
	keywords = {Hypothesis testing, Linear mixed effect model, Power},
	pages = {305--315},
	file = {Matuschek et al_2017_Balancing Type I error and power in linear mixed models.pdf:/mnt/hdd/Dropbox/Zotero/Open science course/Matuschek et al_2017_Balancing Type I error and power in linear mixed models.pdf:application/pdf;ScienceDirect Snapshot:/home/kris/Zotero/storage/2DM89M9N/S0749596X17300013.html:text/html}
}

@article{gueorguieva_move_2004,
	title = {Move over {ANOVA}: progress in analyzing repeated-measures data and its reflection in papers published in the {Archives} of {General} {Psychiatry}},
	volume = {61},
	issn = {0003-990X},
	shorttitle = {Move over {ANOVA}},
	doi = {10.1001/archpsyc.61.3.310},
	abstract = {BACKGROUND: The analysis of repeated-measures data presents challenges to investigators and is a topic for ongoing discussion in the Archives of General Psychiatry. Traditional methods of statistical analysis (end-point analysis and univariate and multivariate repeated-measures analysis of variance [rANOVA and rMANOVA, respectively]) have known disadvantages. More sophisticated mixed-effects models provide flexibility, and recently developed software makes them available to researchers.
OBJECTIVES: To review methods for repeated-measures analysis and discuss advantages and potential misuses of mixed-effects models. Also, to assess the extent of the shift from traditional to mixed-effects approaches in published reports in the Archives of General Psychiatry.
DATA SOURCES: The Archives of General Psychiatry from 1989 through 2001, and the Department of Veterans Affairs Cooperative Study 425.
STUDY SELECTION: Studies with a repeated-measures design, at least 2 groups, and a continuous response variable.
DATA EXTRACTION: The first author ranked the studies according to the most advanced statistical method used in the following order: mixed-effects model, rMANOVA, rANOVA, and end-point analysis.
DATA SYNTHESIS: The use of mixed-effects models has substantially increased during the last 10 years. In 2001, 30\% of clinical trials reported in the Archives of General Psychiatry used mixed-effects analysis.
CONCLUSIONS: Repeated-measures ANOVAs continue to be used widely for the analysis of repeated-measures data, despite risks to interpretation. Mixed-effects models use all available data, can properly account for correlation between repeated measurements on the same subject, have greater flexibility to model time effects, and can handle missing data more appropriately. Their flexibility makes them the preferred choice for the analysis of repeated-measures data.},
	language = {eng},
	number = {3},
	journal = {Archives of General Psychiatry},
	author = {Gueorguieva, Ralitza and Krystal, John H.},
	month = mar,
	year = {2004},
	pmid = {14993119},
	keywords = {Analysis of Variance, Endpoint Determination, Humans, Models, Statistical, Psychiatry, Research Design},
	pages = {310--317}
}

@article{barr_random_2013,
	title = {Random effects structure for confirmatory hypothesis testing: {Keep} it maximal},
	volume = {68},
	issn = {0749-596X},
	shorttitle = {Random effects structure for confirmatory hypothesis testing},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X12001180},
	doi = {10.1016/j.jml.2012.11.001},
	abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the ‘gold standard’ for confirmatory hypothesis testing in psycholinguistics and beyond.},
	number = {3},
	urldate = {2018-03-13},
	journal = {Journal of Memory and Language},
	author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
	month = apr,
	year = {2013},
	keywords = {Generalization, Linear mixed-effects models, Monte Carlo simulation, Statistics},
	pages = {255--278},
	file = {Barr et al_2013_Random effects structure for confirmatory hypothesis testing.pdf:/mnt/hdd/Dropbox/Zotero/Open science course/Barr et al_2013_Random effects structure for confirmatory hypothesis testing.pdf:application/pdf;ScienceDirect Snapshot:/home/kris/Zotero/storage/J2JTS6AC/S0749596X12001180.html:text/html}
}

@article{clark_language-as-fixed-effect_1973,
	title = {The language-as-fixed-effect fallacy: {A} critique of language statistics in psychological research},
	volume = {12},
	issn = {0022-5371},
	shorttitle = {The language-as-fixed-effect fallacy},
	url = {http://www.sciencedirect.com/science/article/pii/S0022537173800143},
	doi = {10.1016/S0022-5371(73)80014-3},
	abstract = {Current investigators of words, sentences, and other language materials almost never provide statistical evidence that their findings generalize beyond the specific sample of language materials they have chosen. Nevertheless, these same investigators do not hesitate to conclude that their findings are true for language in general. In so doing, it is argued, they are committing the language-as-fixed-effect fallacy, which can lead to serious error. The problem is illustrated for one well-known series of studies in semantic memory. With the appropriate statistics these studies are shown to provide no reliable evidence for most of the main conclusions drawn from them. A review of other experiments in semantic memory shows that many of them are likewise suspect. It is demonstrated how this fallacy can be avoided by doing the right statistics, selecting the appropriate design, and sampling by systematic procedures, or, alternatively, by proceeding according to the so-called method of single cases.},
	number = {4},
	urldate = {2018-03-13},
	journal = {Journal of Verbal Learning and Verbal Behavior},
	author = {Clark, Herbert H.},
	month = aug,
	year = {1973},
	pages = {335--359},
	file = {ScienceDirect Snapshot:/home/kris/Zotero/storage/7UK4XFMH/S0022537173800143.html:text/html}
}

@article{wicherts_degrees_2016,
	title = {Degrees of {Freedom} in {Planning}, {Running}, {Analyzing}, and {Reporting} {Psychological} {Studies}: {A} {Checklist} to {Avoid} p-{Hacking}},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {Degrees of {Freedom} in {Planning}, {Running}, {Analyzing}, and {Reporting} {Psychological} {Studies}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5122713/},
	doi = {10.3389/fpsyg.2016.01832},
	abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
	urldate = {2018-03-13},
	journal = {Frontiers in Psychology},
	author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and van Aert, Robbie C. M. and van Assen, Marcel A. L. M.},
	month = nov,
	year = {2016},
	pmid = {27933012},
	pmcid = {PMC5122713},
	file = {Wicherts et al_2016_Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological.pdf:/mnt/hdd/Dropbox/Zotero/Open science course/Wicherts et al_2016_Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological.pdf:application/pdf}
}

@article{kwok_impact_2007,
	title = {The {Impact} of {Misspecifying} the {Within}-{Subject} {Covariance} {Structure} in {Multiwave} {Longitudinal} {Multilevel} {Models}: {A} {Monte} {Carlo} {Study}},
	volume = {42},
	issn = {0027-3171},
	shorttitle = {The {Impact} of {Misspecifying} the {Within}-{Subject} {Covariance} {Structure} in {Multiwave} {Longitudinal} {Multilevel} {Models}},
	url = {https://doi.org/10.1080/00273170701540537},
	doi = {10.1080/00273170701540537},
	abstract = {This Monte Carlo study examined the impact of misspecifying the Σ matrix in longitudinal data analysis under both the multilevel model and mixed model frameworks. Under the multilevel model approach, under-specification and general-misspecification of the Σ matrix usually resulted in overestimation of the variances of the random effects (e.g., τ00, ττ11 ) and standard errors of the corresponding growth parameter estimates (e.g., SEβ 0, SEβ 1). Overestimates of the standard errors led to lower statistical power in tests of the growth parameters. An unstructured Σ matrix under the mixed model framework generally led to underestimates of standard errors of the growth parameter estimates. Underestimates of the standard errors led to inflation of the type I error rate in tests of the growth parameters. Implications of the compensatory relationship between the random effects of the growth parameters and the longitudinal error structure for model specification were discussed.},
	number = {3},
	urldate = {2018-03-13},
	journal = {Multivariate Behavioral Research},
	author = {Kwok, Oi-man and West, Stephen G. and Green, Samuel B.},
	month = oct,
	year = {2007},
	pages = {557--592},
	file = {Snapshot:/home/kris/Zotero/storage/9X4SCKVW/00273170701540537.html:text/html}
}